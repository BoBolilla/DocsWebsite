# convolutional neural network

将AlexNet、VGG和NiN的模型参数大小与GoogLeNet进行比较。后两个网络架构是如何显著减少模型参数大小的？

* **模型参数规模对比**

    | 网络架构   | 参数量（约） | 特点                     |
    |------------|--------------|--------------------------|
    | **AlexNet**| 60 million   | 全连接层占90%以上参数     |
    | **VGG16**  | 138 million  | 深层小卷积核，全连接层庞大 |
    | **NiN**    | 1-3 million  | 全局平均池化替代全连接层  |
    | **GoogLeNet**| 5-7 million| Inception模块 + 1x1卷积降维 |

---

* **NiN和GoogLeNet减少参数的核心方法**
  * **1. NiN（Network in Network）**

    * **全局平均池化（Global Average Pooling）**：  
       直接对最后一个卷积层的每个通道取全局平均值，生成一个与类别数相同的向量（如1000类）。**避免了全连接层**，参数减少90%以上（例如，AlexNet的FC层有约4000万参数，而NiN此部分参数几乎为0）。

    * **MLP卷积层（1x1卷积）**：  
       用多层1x1卷积替代传统卷积层，增强非线性能力的同时减少参数。例如：  
      * 传统3x3卷积：输入256通道→输出256通道，参数为 $3×3×256×256=589,824$。  
      * 1x1卷积：参数仅为 $1×1×256×256=65,536$，减少近9倍。

  * **2. GoogLeNet（Inception v1）**

    * **Inception模块的1x1卷积降维**：  
       在3x3和5x5卷积前插入1x1卷积，减少输入通道数。例如：  
      * 输入256通道 → 1x1卷积降维到64通道 → 3x3卷积输出128通道。  
      * 参数：$1×1×256×64+3×3×64×128=16,384+73,728=90,112$。  
      * 若不用1x1降维，参数为 $3×3×256×128=294,912$，减少约70%。

    * **无全连接层，全局平均池化**：  
       与NiN类似，GoogLeNet也使用全局平均池化代替全连接层，进一步压缩参数。

    * **并行多尺度卷积（Inception结构）**：  
       通过并行不同尺寸的卷积核（如1x1、3x3、5x5）提取多尺度特征，避免堆叠单一卷积导致的参数膨胀。

---

* **总结**

  * **NiN**：通过1x1卷积和全局池化，直接消除全连接层，参数降至百万级。  

  * **GoogLeNet**：利用Inception模块中的1x1降维和全局池化，参数控制在百万级，同时保持高性能。  
  * **对比VGG/AlexNet**：两者仍依赖庞大的全连接层和未优化的卷积结构，导致参数规模显著更大。
