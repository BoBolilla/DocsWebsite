# 索引

## 细粒度分类

### [Fine-Grained Visual Classification via Internal  Ensemble Learning Transformer](./202507/Fine-Grained%20Visual%20Classification%20via%20Internal%20%20Ensemble%20Learning%20Transformer.md)

IEEE Transactions on Multimedia (TMM) ,2023

* **多头注意力弱学习器集成（投票机制）**--->解决层内头间性能不均衡，识别图像更多的局部关键部位，找到重要的关键的特征
* **跨层融合**抑制噪声，增强特征表达，在于如何更好的跨层融合。 --->层间融合性能问题
* 动态调整各层的token选择数量，增强表现好的层，抑制表现差的层

### [TransIFC Invariant Cues-Aware Feature  Concentration Learning for Efficient Fine-Grained  Bird Image Classification](./202507/TransIFC%20Invariant%20Cues-Aware%20Feature%20%20Concentration%20Learning%20for%20Efficient%20Fine-Grained%20%20Bird%20Image%20Classification)

IEEE Transactions on Multimedia (TMM) ,2023

1. 找关键特征--->[**只要输出特征中区分度最大的前K个特征**](./202507/TransIFC%20Invariant%20Cues-Aware%20Feature%20%20Concentration%20Learning%20for%20Efficient%20Fine-Grained%20%20Bird%20Image%20Classification#FFA)：计算每个特征之间的相似度，求和倒数，分数大说明区分度更大，不普通；分数小说明这个特征没有说明特色，不重要。
2. 低层（包含更多细节）和高层（包含更多语义）的[信息融合](./202507/TransIFC%20Invariant%20Cues-Aware%20Feature%20%20Concentration%20Learning%20for%20Efficient%20Fine-Grained%20%20Bird%20Image%20Classification#HSFA)。
3. 鸟类图像中的**不变线索**（除了眼睛鸟喙等还包括**长距离语义关系**，翅膀&鸟喙位置关系）和**细微差异**。

## 推荐

### Sequential Recommendation

> 已知用户的、长期积累的、按时间顺序排列的完整行为序列，来预测该用户下一个可能感兴趣的项目。它旨在建模用户长期且稳定的个性化兴趣。

#### :writing_hand:[Pattern-wise transparent sequential recommendation](./202510/模式级透明序列推荐)

IEEE TKDE，2025，Kun Ma; Cong Xu; Zeyuan Chen; Wei Zhang

**保证性能同时提高可解释性**：从单一项目解释（point-level）到多个**连续项目组合**（union-level）解释目标项目（提高精度，解释能力更强）；使用**概率嵌入**（表达更丰富）的方式建模项目，再使用逻运算组合项目，保证项目组合内部的可解释性（每个物品的贡献值）；没有联合集数据，加入权重偏执让**自适应学习**。

> 阐述一个简单模型，思路清晰，实验完整，可以学习写作方式

####  :writing_hand::heart:[:flags:Modeling dynamic item tendency bias in sequential recommendation with causal intervention](./202510/基于因果干预的序列推荐中动态项目倾向偏见建模.md)

IEEE TKDE ，2024，Modeling dynamic item tendency bias in sequential recommendation with causal intervention

对于推荐序列提出了一种新偏差"动态项目倾向偏差"，使用**因果干预**方法取出混杂干扰因素，并合理利用动态倾向中有利的部分

> 引言我很喜欢，可以模仿，适合“**提出一个新问题并解决**”这样的文章写作
>
> 以及rw，当前该方向有哪几类并展开解释，很标准感觉，适合仿写
>
> 图表解释

#### [Preference-consistent knowledge distillation for recommender system](./202510/推荐系统中偏好一致性知识蒸馏.md)

**IEEE TKDE**，2024，Zhangchi Zhu; Wei Zhang

加入两个正则化使学生模型和教师模型之间在投影前特征偏好一致

### Session-based Recommendation

> 输入是用户在当前一次交互“会话”中的匿名行为序列。
>
> 短期性、实时性、匿名性、动态演化性

#### [Explainable Session-based Recommendation  via Path Reasoning](./202510/基于路径推理的可解释会话推荐.md)

IEEE TKDE, 2024, Yang Cao; Shuo Shang; Jun Wang; Wei Zhang

可解释推荐、基于会话的推荐、分层强化学习（融合市面上方法的优点）、知识图谱（针对起点单一找多个起点）；**利用大模型api提取图像特征**（额外信息）加入到训练中；

> **多个模块合并形成一个大的模块**，如何进行**实验**（总体、消融、参数、解释）可参考这一篇

### other

#### [ UNDERSTANDING GENERATIVE RECOMMENDATION  WITH SEMANTIC IDS FROM A MODEL-SCALING VIEW](./202511/SID-basedvsLLM-as-RS模型缩放角度.md)

SID-based  vs LLM-as-RS  从模型缩放角度：

1. SID-based方法因为**LLM嵌入**提取到**离散的SID**中会丢弃大量的语义信息，所以会出现模型规模增大但性能却饱和无法提升的现象
2.  证明其优越缩放属性LLM-as-RS，能在相同数据下提升性能高达20%，且证明了LLMs可以学CF，提出在资源充足的情况下这个方法更有前景

> 后面的补充实验很多，可以模仿实验
>
> 概念:  scaling-law  Collaborative Filtering

## 知识追踪

### :flags:[Rebalancing Discriminative Responses for Knowledge Tracing](./202510/知识追踪中的判别响应再平衡.md)

ACM TOIS，2025， Jiajun Cui; Hong Qian; Chanjin Zheng; Lu Wang; Mo Yu; Wei Zhang

不平衡的数据分布：关注题目中判别力更强的信息，例如简单题中答错的人，难题中答对的人

### [Improving the performance and explainability of knowledge tracing via markov blanket](./202510/通过MarkovBlanket提升知识追踪的性能和可解释性.md)

IPM；Bo Jiang; Yuang Wei; Ting Zhang; Wei Zhang，2024

可解释性：相关性===>**因果关系** （选择合适的输入特征）;

对于知识追踪技术，提出了一种基于markov blanket和FGES框架的算法，提取出对目标参数的一个**因果特征子集（MB）**（有**因果关系**），将该自己输入到一些可解释性强的模型（LR、RF、DT）进行知识追踪。**方法简单有效**

> **实验数据分析**和讨论（**Discussion**）部分写的好，可以参考

## 其他

### [Understanding Adversarial Robustness from Feature  Maps of Convolutional Layers](./202510/Understanding Adversarial Robustness from Feature  Maps of Convolutional Layers)

IEEE TNNLS,Cong Xu; Wei Zhang; Jun Wang; Min Yang,2024

对于平均池化层，**扩大其特征图尺寸**（上采样、缩短步长）可以提到网络对抗鲁棒性。**方法简单有效**

> 实验设计完善（对比实验、展示方法优势），可以参考实验思路

### [Layer by Layer Uncovering Hidden Representations in Language Models](./202507/Layer%20by%20Layer%20Uncovering%20Hidden%20Representations%20in%20Language%20Models.md)

ICML, Oscar Skean 、Md Rifat Arefin、Dan Zhao 、 Niket Patel 、Jalal Naghiyev 、Yann LeCun 、Ravid Shwartz-Ziv, 2025.2

用一个统一框架矩阵熵（信息论、几何、不变性  ）来证明中间层比最终层提供更有用的特征对于下游任务：中间层在**信息压缩和噪声抑制方面**找到了更好的平衡点，而最后一层可能会变得过于专业化于预训练目标

### LiDAR视觉定位

#### [LightLoc Learning Outdoor LiDAR Localization at Light Speed](./202507/LightLoc%20Learning%20Outdoor%20LiDAR%20Localization%20at%20Light%20Speed.md)

cvpr2025, xmu

轻量化

* 多种不同场景训练一个主干网络，N个MLP，每个MLP对于一个场景，并行训练迫使主干网络学到通用的泛化特征。
* **很多样本相似，容易混淆**，模型训练不易收敛：给模型特征加一个自己**大概在某个位置**先验条件。将样本聚类后重新定义标签，做一个简单的分类任务，得到一个条件概率，拼接到样本特征上。
* **样本中存在大量重复**，有些样本模型反复学过，可以不要。计算epoch间一个滑动窗口内损失的方差，如果方差大说明模型预测不稳定，还要学；反之，不用学可以丢掉。方差降序筛选。

## 解释

 :flags:unfinished

:question: leave a question open

:heart: like

:writing_hand: writing
