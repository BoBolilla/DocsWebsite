# 损失函数

`ai生成内容`

损失函数（Loss Function）是衡量模型预测值（Prediction）与真实值（Target / Ground Truth）之间差异的函数。它是模型优化的目标，通过最小化损失函数来训练模型。

损失函数大致可以分为三类：**回归损失**、**分类损失**和**其他特殊任务的损失**。

------

### 一、回归任务（Regression）的损失函数

回归问题预测的是连续值。

#### 1. 均方误差（Mean Square Error, MSE / L2 Loss）

- **公式**：$$MSE = \frac{1}{n} \sum_{i=1}^{n}(y_i - \hat{y_i})^2$$
- **解释**：计算预测值与真实值之差的平方的平均值。平方放大了较大误差的惩罚，因此对异常值（Outliers）比较敏感。
- **特点**：曲线是光滑可导的，易于优化。

#### 2. 平均绝对误差（Mean Absolute Error, MAE / L1 Loss）

- **公式**：$$MAE = \frac{1}{n} \sum_{i=1}^{n}|y_i - \hat{y_i}|$$
- **解释**：计算预测值与真实值之差的绝对值的平均值。相比MSE，它对异常值不那么敏感。
- **特点**：在零点处不可导，优化效率可能不如MSE。

#### 3. Huber Loss（平滑平均绝对误差）

- **公式**：
  - $$L_{\delta} =\begin{cases}\frac{1}{2}(y - \hat{y})^2 & \text{for } |y - \hat{y}| \leq \delta \\delta |y - \hat{y}| - \frac{1}{2}\delta^2 & \text{for } |y - \hat{y}| >\delta\end{cases}$$
- **解释**：Huber Loss 结合了 MSE 和 MAE 的优点。在误差较小时，它表现为 MSE，光滑易导；在误差较大时，它表现为 MAE，对异常值更鲁棒。δ是一个超参数，用于定义“大误差”和“小误差”的阈值。
- **特点**：兼具MSE和MAE的优点，是两者的折中方案。

------

### 二、分类任务（Classification）的损失函数

分类问题预测的是离散的类别。

#### 1. 交叉熵损失（Cross-Entropy Loss）

这是分类任务中最核心、最常用的损失函数。

- **二分类交叉熵损失（Binary Cross-Entropy）**
  - **公式**：$$BCE = -\frac{1}{n} \sum_{i=1}^{n} \left[ y_i \cdot \log(\hat{y_i}) + (1 - y_i) \cdot \log(1 - \hat{y_i}) \right]$$
  - **解释**：$y_i$是真实标签（0或1），$$\hat{y_i}$$是模型预测为正类的概率（0到1之间）。它衡量了预测概率分布与真实分布之间的差距。
  - **应用**：二分类问题，如判断“是/否”。
- **多分类交叉熵损失（Categorical Cross-Entropy）**
  - **公式**：$$CCE = -\frac{1}{n} \sum_{i=1}^{n} \sum_{c=1}^{C} y_{i,c} \cdot \log(\hat{y_{i,c}})$$
  - **解释**：C是类别总数。$y_{i,c}$是样本 i在类别 c上的真实标签（One-Hot编码），$\hat{y_{i,c}}$是模型预测样本 i属于类别 c的概率。
  - **应用**：多分类问题，如图像分类（猫、狗、汽车...）。

#### 2. 合页损失（Hinge Loss）

- **公式**：$$L = \frac{1}{n} \sum_{i=1}^{n} \max(0, 1 - y_i \cdot \hat{y_i})$$
  - 这里的 $y_i$是真实标签，取值为 **+1** 或 **-1**。$\hat{y_i}$是模型的原始输出（决策函数的分数），而不是概率。
- **解释**：合页损失不仅要求分类正确，还要求有足够大的“**置信度间隔**”（Margin）。只有当 $y_i \cdot \hat{y_i}>=1$时，损失才为0。
- **应用**：主要用于传统的支持向量机（SVM）。

------

### 三、其他任务的损失函数

#### 1. 对比损失（Contrastive Loss）和三元组损失（Triplet Loss）

- **用途**：用于**度量学习（Metric Learning）**，目标是学习一个嵌入空间（Embedding Space），使得相似样本在这个空间中距离很近，不相似样本距离很远。
- **例子**：人脸识别、图像检索。它们通过比较样本对（对比损失）或三元组（Anchor, Positive, Negative）来计算损失。

#### 2. Focal Loss

- **用途**：是交叉熵损失的改进版本，用于解决**类别不平衡**问题（例如目标检测任务中，背景像素远多于目标像素）。
- **特点**：它通过降低“容易分类的样本”的权重，让模型更专注于学习“难分类的样本”。

### 总结

| 损失函数       | 主要应用              | 特点                                 |
| -------------- | --------------------- | ------------------------------------ |
| **MSE (L2)**   | 回归                  | 对异常值敏感，光滑易优化             |
| **MAE (L1)**   | 回归                  | 对异常值不敏感，零点不可导           |
| **Huber**      | 回归                  | MSE和MAE的折中，鲁棒且可导           |
| **交叉熵**     | 分类（二分类/多分类） | 分类任务的核心标准，衡量概率分布差异 |
| **合页损失**   | 分类（SVM）           | 追求最大间隔的分类边界               |
| **Focal Loss** | 分类（类别不平衡）    | 交叉熵的改进，解决样本不平衡问题     |

选择哪种损失函数取决于具体的任务、数据分布以及对异常值的敏感度。在实践中，**MSE** 和 **交叉熵** 是最常用和最重要的基础损失函数。