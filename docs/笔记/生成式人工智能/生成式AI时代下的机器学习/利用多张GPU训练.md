# åˆ©ç”¨å¤šå¼ GPUè®­ç»ƒ

[åŠ©æ•™è¯¾ï¼šåˆ©ç”¨å¤šå¼ GPUè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹â€”ä»é›¶å¼€å§‹ä»‹ç»DeepSpeedã€Liger Kernelã€Flash Attension_å“”å“©å“”å“©_bilibili](https://www.bilibili.com/video/BV1aiADewEBC?spm_id_from=333.788.videopod.episodes&vd_source=4e1dceccc918063def66c9d643674c6a&p=13)

> åŠ©æ•™è¶Šçœ‹è¶Šå¸….......

è¿™èŠ‚ä¸»è¦è®²å¦‚ä½•é™æ˜¾å­˜

## Overview



![image-20251102161413732](./assets/image-20251102161413732.png)

## Introduce

### Forwarded & Backward pass

<img src="./assets/image-20251102102222577.png" alt="image-20251102102222577" style="zoom:67%;" />

### Memory needed

![image-20251102102318487](./assets/image-20251102102318487.png)

è¦ä¼°ç®—è®­ç»ƒä¸€ä¸ª 8Bï¼ˆ80 äº¿å‚æ•°ï¼‰æ¨¡å‹æ‰€éœ€çš„ GPU å†…å­˜ï¼ˆæ˜¾å­˜ï¼‰(ä¸è€ƒè™‘è¾“å…¥ï¼Œä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ + Adam ä¼˜åŒ–å™¨ï¼‰ï¼š

> â€œä¸è€ƒè™‘è¾“å…¥â€ï¼Œå°±æš‚æ—¶å¿½ç•¥æ¿€æ´»å†…å­˜
>
> æ¿€æ´»å€¼å†…å­˜å–å†³äº batch sizeã€åºåˆ—é•¿åº¦ã€æ¨¡å‹ç»“æ„ç­‰

#### 1. **æ¨¡å‹å‚æ•°ï¼ˆParametersï¼‰**

- æ¯ä¸ªå‚æ•°åœ¨æ··åˆç²¾åº¦è®­ç»ƒä¸­é€šå¸¸ä»¥ **float16 (2 bytes)** å­˜å‚¨ç”¨äºå‰å‘/åå‘è®¡ç®—ã€‚
- ä½†ä¸ºäº†ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ›´æ–°ï¼Œé€šå¸¸ä»éœ€ä¿ç•™ä¸€ä»½ **float32 (4 bytes)** çš„ä¸»å‰¯æœ¬ã€‚

æ‰€ä»¥ï¼š
- float16 å‚æ•°ï¼š8B Ã— 2 bytes = **16 GB**
- float32 ä¸»å‚æ•°ï¼ˆç”¨äºä¼˜åŒ–å™¨ï¼‰ï¼š8B Ã— 4 bytes = **32 GB**

> æ³¨æ„ï¼šæœ‰äº›å®ç°ï¼ˆå¦‚ PyTorch çš„ AMP + Adamï¼‰ä¼šå°†æ¨¡å‹å‚æ•°æœ¬èº«ä¿æŒä¸º float32ï¼Œä½†åœ¨å‰å‘/åå‘ä¸­ä½¿ç”¨ float16 çš„å‰¯æœ¬ã€‚å› æ­¤ï¼Œå‚æ•°æœ¬èº«é€šå¸¸ä»å  32 GBã€‚

---

#### 2. **ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆAdamï¼‰**

Adam ä¼˜åŒ–å™¨ä¸ºæ¯ä¸ªå‚æ•°ç»´æŠ¤ä¸¤ä¸ªçŠ¶æ€ï¼š**momentum (m)** å’Œ **variance (v)**ï¼Œé€šå¸¸éƒ½ç”¨ **float32**ã€‚
- æ¯ä¸ªå‚æ•°éœ€è¦ 2 Ã— 4 = 8 bytes
- æ€»è®¡ï¼š8B Ã— 8 bytes = **64 GB**

#### 3. **æ¢¯åº¦ï¼ˆGradientsï¼‰**
- æ¢¯åº¦é€šå¸¸ä»¥ **float32** å­˜å‚¨ï¼ˆå³ä½¿åœ¨æ··åˆç²¾åº¦ä¸­ï¼Œæ¢¯åº¦ä¹Ÿæ˜¯ float32 ç”¨äºæ›´æ–°ï¼‰ã€‚
- 8B Ã— 4 bytes = **32 GB**

---

#### 4. **ä¸´æ—¶ç¼“å†²åŒºå’Œå…¶ä»–å¼€é”€**
- é€šå¸¸é¢å¤–å¢åŠ  10â€“20% çš„å¼€é”€ï¼ˆå¦‚é€šä¿¡ç¼“å†²åŒºã€ä¸´æ—¶å¼ é‡ç­‰ï¼‰ã€‚

### activations

![image-20251102113257284](./assets/image-20251102113257284.png)

####  å‡è®¾æ¡ä»¶ï¼ˆåŸºäºå…¸å‹ Transformer æ¶æ„ï¼‰

- æ¨¡å‹æ€»å‚æ•° â‰ˆ 8Bï¼ˆ80 äº¿ï¼‰
- å±‚æ•°ï¼š**L = 32**
- éšè—å±‚ç»´åº¦ï¼ˆhidden sizeï¼‰ï¼šè®°ä¸º **d_model**
- æ³¨æ„åŠ›å¤´æ•°ã€FFN æ‰©å±•æ¯”ä¾‹ç­‰æŒ‰æ ‡å‡†è®¾è®¡
- è¾“å…¥ token æ•°é‡ï¼ˆå³åºåˆ—é•¿åº¦ Ã— batch sizeï¼‰ï¼šè®°ä¸º **n**
- ä½¿ç”¨ **æ··åˆç²¾åº¦ï¼ˆfloat16 / bfloat16ï¼‰** å­˜å‚¨æ¿€æ´»å€¼ â†’ **æ¯ä¸ªæ¿€æ´»å€¼å  2 bytes**
- æš‚ä¸è€ƒè™‘ LayerNormã€bias ç­‰å°é¡¹ï¼ˆå¯å¿½ç•¥æˆ–åŠ  10% ä½™é‡ï¼‰

#### ç¬¬ä¸€æ­¥ï¼šä¼°ç®— d_modelï¼ˆéšè—ç»´åº¦ï¼‰

> å¯¹äº 8B å‚æ•°çš„ 32 å±‚ Transformerï¼Œå¯ä»¥åæ¨å…¸å‹ d_model

ä¸€ä¸ªæ ‡å‡†çš„ decoder-only Transformerï¼ˆå¦‚ LLaMA æ¶æ„ï¼‰æ¯å±‚å‚æ•°ä¸»è¦æ¥è‡ªï¼š

1. **Attention**ï¼ˆå¤šå¤´ï¼‰ï¼šQ, K, V, O æŠ•å½±ï¼š4 Ã— d_modelÂ²
2. **Feed-Forward Network (FFN)**ï¼š
   - é€šå¸¸ä¸­é—´ç»´åº¦ä¸º **4 Ã— d_model**ï¼ˆå³ ffn_dim = 4dï¼‰
   - ä¸¤ä¸ªçº¿æ€§å±‚ï¼šd_model Ã— 4d_model + 4d_model Ã— d_model = 8 Ã— d_modelÂ²

æ‰€ä»¥æ¯å±‚å‚æ•° â‰ˆ  â€ƒ**4dÂ² + 8dÂ² = 12dÂ²**

32 å±‚æ€»å‚æ•° â‰ˆ 32 Ã— 12dÂ² = **384 dÂ²**

è®¾æ€»å‚æ•° â‰ˆ 8 Ã— 10â¹ï¼š

$$
384 d^2 \approx 8 \times 10^9 \Rightarrow d^2 \approx \frac{8 \times 10^9}{384} \approx 20.83 \times 10^6 \Rightarrow d \approx \sqrt{20.83 \times 10^6} \approx 4564
$$

æ‰€ä»¥ **d_model â‰ˆ 4096 ~ 4608** æ˜¯åˆç†çš„ï¼Œå–ä¸€ä¸ªå¸¸è§å€¼ï¼š**d_model = 4096**

> ï¼ŒLLaMA-7B ä½¿ç”¨ d_model = 4096ï¼Œ32 å±‚ï¼Œ â‰ˆ7B å‚æ•°ï¼Œæ¥è¿‘8Bï¼Œå› æ­¤è¿™ä¸ªå‡è®¾åˆç†ã€‚

#### ç¬¬äºŒæ­¥ï¼šæ¯å±‚æ¿€æ´»å€¼ä¼°ç®—

åœ¨è®­ç»ƒæ—¶ï¼Œä¸ºäº†åå‘ä¼ æ’­ï¼Œéœ€è¦ä¿å­˜å‰å‘ä¼ æ’­ä¸­çš„ä¸­é—´æ¿€æ´»ã€‚ä¸»è¦ä¿å­˜çš„æœ‰ï¼š

##### å¯¹äºæ¯ä¸ª Transformer å±‚ï¼ˆdecoder-onlyï¼‰ï¼š
1. **Attention è¾“å…¥**ï¼š(n, d) â†’ ä¿å­˜ç”¨äºæ®‹å·®è¿æ¥å’Œ LayerNorm
2. **Q, K, V æŠ•å½±è¾“å‡º**ï¼šå„ (n, d) â†’ æœ‰äº›å®ç°ä¼šä¿å­˜ï¼Œæœ‰äº›ä¼š recomputed
3. **Attention è¾“å‡º**ï¼š(n, d)
4. **FFN è¾“å…¥**ï¼š(n, d)
5. **FFN ä¸­é—´æ¿€æ´»ï¼ˆå¦‚ SiLU/GELU åï¼‰**ï¼š(n, 4d) â† **è¿™æ˜¯æœ€å¤§çš„éƒ¨åˆ†**
6. **FFN è¾“å‡º**ï¼š(n, d)

ä½†ä¸ºäº†ç®€åŒ–ï¼Œ**å¸¸ç”¨ç»éªŒå…¬å¼**æ˜¯ï¼š

> æ¯å±‚æ¿€æ´» â‰ˆ **(n Ã— d_model) Ã— C**ï¼Œå…¶ä¸­ C æ˜¯ä¸€ä¸ªå¸¸æ•°ï¼ˆé€šå¸¸å– 6~10ï¼‰

==æœ‰æ—¶è¿˜åŒ…æ‹¬ attention softmax å‰çš„ logitsï¼šn Ã— nï¼Œä½†è‹¥ n å¾ˆå¤§åˆ™ä¸å¯å¿½ç•¥==

ä½† **n Ã— n çš„ attention score é€šå¸¸ä¸ä¿å­˜**ï¼ˆå› ä¸ºåå‘æ—¶å¯ recomputed æˆ–ç”¨ flash attentionï¼‰ï¼Œæ‰€ä»¥æˆ‘ä»¬å…ˆå¿½ç•¥ O(nÂ²) é¡¹ï¼Œé™¤é n > 2048ã€‚

å› æ­¤ï¼Œä¸»è¦æ¿€æ´»ä¸ºï¼š
$$
\text{æ¯å±‚æ¿€æ´»å…ƒç´ æ•°} \approx n \cdot d + n \cdot d + n \cdot 4d = 6 n d
$$
ä»¥ float16 å­˜å‚¨ â†’ æ¯å…ƒç´  2 bytesï¼š$$
\text{æ¯å±‚æ¿€æ´»å†…å­˜} = 6 n d \times 2 = 12 n d \text{ bytes}
$$

æ€»å±‚æ•° L = 32ï¼š$$
\text{æ€»æ¿€æ´»å†…å­˜} = 32 \times 12 n d = 384 \, n d \text{ bytes}
$$

ä»£å…¥ d = 4096ï¼š

$$
\text{æ¿€æ´»å†…å­˜} = 384 \times n \times 4096 \text{ bytes} = 1,572,864 \, n \text{ bytes} \approx 1.57 \, n \text{ MB}
$$

> ğŸ’¡ å¦‚æœä½¿ç”¨ **æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆactivation checkpointingï¼‰**ï¼Œå¯ä»¥åªä¿å­˜æ¯è‹¥å¹²å±‚çš„æ¿€æ´»ï¼ˆå¦‚æ¯å±‚éƒ½ checkpointï¼‰ï¼Œæ­¤æ—¶æ¿€æ´»å†…å­˜å¯é™è‡³ **â‰ˆ 2~3 å±‚çš„æ¿€æ´»é‡**ï¼Œå³ï¼š
> $$
> \text{Checkpointed æ¿€æ´»} \approx 3 \times 12 n d = 36 n d \text{ bytes} \approx 0.15 \, n \text{ MB}
> $$
> ä¾‹å¦‚ n=2M æ—¶ï¼Œä»…éœ€ ~300 MB æ¿€æ´»å†…å­˜ï¼ˆä½†**åå‘ä¼ æ’­æ—¶é—´å¢åŠ çº¦ 30%**ï¼‰ã€‚

### batch size

![image-20251102113600867](./assets/image-20251102113600867.png)

å…¨å±€ batch sizeï¼Œå³æ¯æ­¥æ›´æ–°æ‰€ç”¨çš„æ€» token æ•°

| æœ¯è¯­                                 | å«ä¹‰                                                         |
| ------------------------------------ | ------------------------------------------------------------ |
| **Mini-batch size**                  | å•æ¬¡å‰å‘/åå‘ä¼ æ’­å¤„ç†çš„æ ·æœ¬æ•°ï¼ˆå— GPU æ˜¾å­˜é™åˆ¶ï¼‰             |
| **Gradient accumulation steps (GA)** | å¤šå°‘ä¸ª mini-batch çš„æ¢¯åº¦ç´¯åŠ åæ‰åšä¸€æ¬¡å‚æ•°æ›´æ–°               |
| **Global batch size (GBS)**          | æ¯æ¬¡å‚æ•°æ›´æ–°å®é™…ä½¿ç”¨çš„æ€» token æ•° = `mini-batch tokens Ã— GA` |

>  åœ¨å¤§æ¨¡å‹è®­ç»ƒä¸­ï¼Œ**çœŸæ­£å½±å“ä¼˜åŒ–æ•ˆæœçš„æ˜¯ Global Batch Sizeï¼ˆä»¥ token è®¡ï¼‰**ï¼Œè€Œä¸æ˜¯ mini-batchã€‚

---

####  2. ä¸ºä»€ä¹ˆéœ€è¦å¤§çš„ Global Batch Sizeï¼‰ï¼Ÿ

##### ï¼ˆ1ï¼‰æ¢¯åº¦å™ªå£°é™ä½:star2:
- å° batch â†’ æ¢¯åº¦æ–¹å·®å¤§ â†’ è®­ç»ƒä¸ç¨³å®šã€æ”¶æ•›æ…¢
- å¤§ batch â†’ æ¢¯åº¦æ›´æ¥è¿‘çœŸå®æœŸæœ› â†’ æ›´å¹³æ»‘çš„ loss ä¸‹é™
- ç»éªŒè¡¨æ˜ï¼š**LLM è®­ç»ƒä¸­ï¼ŒGBS < 1M tokens æ—¶æ¢¯åº¦å™ªå£°æ˜¾è‘—**

##### ï¼ˆ2ï¼‰ä¸å­¦ä¹ ç‡ååŒç¼©æ”¾ï¼ˆLearning Rate Scalingï¼‰
- å¤§ batch å…è®¸ä½¿ç”¨æ›´å¤§çš„å­¦ä¹ ç‡ï¼ˆçº¿æ€§ç¼©æ”¾è§„åˆ™ï¼‰
- ä¾‹å¦‚ï¼šbatch size Ã—4 â†’ LR Ã—4ï¼ˆåœ¨ä¸€å®šèŒƒå›´å†…æœ‰æ•ˆï¼‰
- è¿™èƒ½åŠ é€Ÿæ”¶æ•›ï¼Œæå‡æœ€ç»ˆæ€§èƒ½

##### ï¼ˆ3ï¼‰ç¡¬ä»¶åˆ©ç”¨ç‡é«˜
- å¤§ batch èƒ½æ›´å¥½åœ°åˆ©ç”¨ GPU è®¡ç®—å•å…ƒï¼ˆæé«˜ FLOPs åˆ©ç”¨ç‡ï¼‰
- å‡å°‘é€šä¿¡/è°ƒåº¦å¼€é”€å æ¯”ï¼ˆå°¤å…¶åœ¨å¤šæœºè®­ç»ƒä¸­ï¼‰

> ğŸ“Œ **Chinchilla è®ºæ–‡å»ºè®®**ï¼šæœ€ä¼˜è®¡ç®—åˆ†é…ä¸‹ï¼Œ**æ€»è®­ç»ƒ token æ•° â‰ˆ 20Ã— æ¨¡å‹å‚æ•°**  
> å¯¹äº 8B æ¨¡å‹ â†’ æ€»è®­ç»ƒ token â‰ˆ 160B  ï¼Œè‹¥æ¯ step ç”¨ 6M tokens â†’ éœ€çº¦ **26,666 æ­¥**

è™½ç„¶ **global batch size å†³å®šä¼˜åŒ–è´¨é‡**ï¼Œä½† **æ˜¾å­˜åªå— micro-batch å½±å“**ï¼

##### æ˜¾å­˜ä¸»è¦ç”±ä»¥ä¸‹å†³å®šï¼š
- **Micro-batch çš„ token æ•°**ï¼ˆ= per-GPU batch size Ã— sequence lengthï¼‰
- æ¿€æ´»å€¼å†…å­˜ âˆ micro-batch tokensï¼ˆè§å‰æ–‡å…¬å¼ï¼šâ‰ˆ1.57 Ã— n MBï¼Œn æ˜¯ per-GPU tokensï¼‰
- ä¼˜åŒ–å™¨çŠ¶æ€ã€å‚æ•°ã€æ¢¯åº¦ä¸ batch æ— å…³ï¼ˆå›ºå®šï¼‰

> **ç­–ç•¥**ï¼šä¸ºäº†è¾¾åˆ°å¤§ GBSï¼Œ**å‡å° micro-batchï¼Œå¢å¤§ GA**ï¼Œä»è€Œæ§åˆ¶ per-GPU æ˜¾å­˜ã€‚

ä½†æ³¨æ„ï¼š
- GA è¿‡å¤§ä¼šå»¶é•¿è®­ç»ƒæ—¶é—´ï¼ˆå› ä¸ºæ¯æ­¥è¦è·‘ GA æ¬¡å‰å‘/åå‘æ‰æ›´æ–°ï¼‰
- é€šä¿¡å¼€é”€å¢åŠ ï¼ˆæ¯ micro-step éƒ½è¦ all-reduce æ¢¯åº¦ï¼‰

> æ‰€ä»¥å®è·µçš„æ—¶å€™éœ€è¦**å…ˆç¡®å®šç¡¬ä»¶èƒ½æ”¯æŒçš„æœ€å¤§ micro-batch**ï¼ˆè€ƒè™‘ seq_len å’Œæ¿€æ´»å†…å­˜ï¼‰

### challenge

![image-20251102122311548](./assets/image-20251102122311548.png)

parameters, gradients and optimizer å ç”¨å†…å­˜å¤§å°ä¸ä¼šéšè¾“å…¥tokenå¤§å°æ”¹å˜è€Œæ”¹å˜ï¼Œä½†æ˜¯activationsä¼šéšç€tokenåŠ å¤§è€ŒæŒ‡æ•°çº§å¢é•¿ã€‚



## Part I : parameters, gradients and optimizer

###  How to leverage multiple GPUs?

* We have to compute billions of optimization steps, with large batch size 
* The model may be too large to fit in a single GPU
* The input can be long. Self-attention takes O(N^2) memory.

![image-20251102145947773](./assets/image-20251102145947773.png)

### DeepSpeed - Zero Redundancy Optimizer (ZeRO)

å¾®è½¯å¼€å‘çš„ä¸€å¥—é«˜æ•ˆåˆ†å¸ƒå¼è®­ç»ƒä¼˜åŒ–æŠ€æœ¯ã€‚

åœ¨æ ‡å‡† **æ•°æ®å¹¶è¡Œï¼ˆData Parallelism, DPï¼‰** ï¼Œæ¯ä¸ª GPU éƒ½ä¿å­˜ä¸€ä»½å®Œæ•´çš„æ¨¡å‹å‚æ•°ã€æ¢¯åº¦ã€ä¼˜åŒ–å™¨çŠ¶æ€å³ä½¿ä½ æœ‰ 8 å¼  A100ï¼ˆ80GBï¼‰ï¼Œä¹Ÿæ— æ³•è®­ç»ƒâ€”â€”å› ä¸ºæ¯å¼ å¡éƒ½è¦å­˜å®Œæ•´å‰¯æœ¬ï¼›**ZeRO çš„æ ¸å¿ƒæ€æƒ³**ï¼š  å°†ä¼˜åŒ–å™¨çŠ¶æ€ã€æ¢¯åº¦ã€å‚æ•°åœ¨ **DP è¿›ç¨‹é—´åˆ†ç‰‡ï¼ˆshardï¼‰**ï¼Œæ¯ä¸ª GPU åªå­˜è‡ªå·±è´Ÿè´£çš„é‚£ä¸€éƒ¨åˆ†ï¼Œä»è€Œçº¿æ€§é™ä½æ˜¾å­˜ã€‚

- **ZeRO-1**ï¼šåˆ†ä¼˜åŒ–å™¨  ï¼ˆé€šè¿‡ä¹‹å‰çš„åˆ†æçŸ¥é“ parameters, gradients and optimizerä¸­å æ˜¾å­˜æœ€å¤§çš„æ˜¯optimizerï¼‰

  ![image-20251102150642128](./assets/image-20251102150642128.png)

- **ZeRO-2**ï¼š+ åˆ†æ¢¯åº¦ â†’ è¿›ä¸€æ­¥çœæ˜¾å­˜  

  ![image-20251102150700890](./assets/image-20251102150700890.png)

- **ZeRO-3**ï¼š+ åˆ†å‚æ•° 

  â€‹	![image-20251102150713105](./assets/image-20251102150713105.png)

  GPUä¹‹é—´ä¼ è¾“æ•ˆç‡å¾ˆé«˜ï¼Œå› æ­¤ä½¿ç”¨zeroé€Ÿåº¦ä¸ä¼šå˜å¾ˆæ…¢

![image-20251102150912523](./assets/image-20251102150912523.png)

- **Offload**ï¼šç”¨ CPU/NVMe æ¢ GPU æ˜¾å­˜ï¼Œç‰ºç‰²é€Ÿåº¦æ¢å¯è¡Œæ€§ï¼ˆGPUã€CPUä¹‹ä¼ è¾“é€Ÿç‡å¾ˆæ…¢ï¼Œä¸æ¨èä½¿ç”¨ï¼‰  

<img src="./assets/image-20251102151027735.png" alt="image-20251102151027735" style="zoom:50%;" />

<img src="./assets/image-20251102151119887.png" alt="image-20251102151119887" style="zoom: 33%;" /><img src="./assets/image-20251102151127884.png" alt="image-20251102151127884" style="zoom:33%;" />

## Part II : activations

å¯¹äºactivationsï¼Œä¸€èˆ¬æ˜¯é€šè¿‡æ”¹å†™kernelæ¥è§£å†³æ˜¾å­˜å ç”¨è¿‡å¤šçš„é—®é¢˜

<img src="./assets/image-20251102152154028.png" alt="image-20251102152154028" style="zoom:50%;" />

 â€œ**kernel**â€ å¹¶ä¸æ˜¯ç¥ç»ç½‘ç»œç»“æ„ä¸­çš„æŸä¸€å±‚ï¼ˆæ¯”å¦‚å·ç§¯å±‚ã€Attention å±‚ï¼‰ï¼Œè€Œæ˜¯æŒ‡ **åº•å±‚æ‰§è¡Œè®¡ç®—çš„åŸºæœ¬å‡½æ•°å•å…ƒ** â€”â€” ä¹Ÿå°±æ˜¯ **åœ¨ GPU ä¸Šå®é™…è¿è¡Œçš„ã€å®Œæˆå…·ä½“æ•°å­¦è¿ç®—çš„ä»£ç ç‰‡æ®µ**ã€‚

| é«˜å±‚æ“ä½œï¼ˆPyTorchï¼‰  | å¯¹åº”çš„åº•å±‚ kernel å¯èƒ½æ˜¯                                     |
| -------------------- | ------------------------------------------------------------ |
| `torch.matmul(A, B)` | ä¸€ä¸ªè°ƒç”¨ cuBLAS çš„ GEMM kernel                               |
| `F.silu(x)`          | ä¸€ä¸ª element-wise çš„ CUDA/Triton kernel                      |
| Self-Attention       | ç”±å¤šä¸ª kernel ç»„æˆï¼šQKV æŠ•å½±ï¼ˆmatmulï¼‰ã€softmaxã€attention outputï¼ˆmatmulï¼‰ç­‰ |

 â€œkernelâ€ å¯ä»¥æœ‰ä¸åŒå®ç°æ–¹å¼ï¼š

| å®ç°æ–¹å¼            | ç¤ºä¾‹                                                         |
| ------------------- | ------------------------------------------------------------ |
| **Naive PyTorch**   | è°ƒç”¨ `torch.softmax`ï¼Œå†…éƒ¨ç”¨é€šç”¨ CUDA kernel                 |
| **torch.compile()** | è‡ªåŠ¨èåˆ `matmul + scaling + softmax` æˆä¸€ä¸ª kernelï¼Œå‡å°‘ä¸­é—´æ˜¾å­˜ |
| **Triton**          | æ‰‹å†™ä¸€ä¸ª fused attention kernelï¼ˆå¦‚ FlashAttention çš„ Triton ç‰ˆï¼‰ |
| **CUDA**            | NVIDIA å·¥ç¨‹å¸ˆå†™çš„ cuBLAS GEMMï¼Œæˆ– FlashAttention-2 çš„ hand-tuned CUDA |

 **å¤§æ¨¡å‹è®­ç»ƒ/æ¨ç†çš„ç“¶é¢ˆå¾€å¾€ä¸åœ¨â€œç®—æ³•â€ï¼Œè€Œåœ¨â€œè®¡ç®—æ•ˆç‡â€**ï¼š

- ä¸€ä¸ªä½æ•ˆçš„ softmax kernel â†’ æ˜¾å­˜çˆ†ç‚¸ã€é€Ÿåº¦æ…¢
- ä¸€ä¸ªèåˆçš„ matmul+SiLU kernel â†’ å‡å°‘ 50% æ˜¾å­˜è¯»å†™ï¼Œæé€Ÿ 2 å€
- ä½¿ç”¨ Tensor Core çš„ CUDA kernel â†’ å……åˆ†åˆ©ç”¨ A100 çš„ç¡¬ä»¶åŠ é€Ÿ

> ç›®æ ‡å‡å°‘kernel

### Flash Attention Algorithm

<img src="./assets/image-20251102152942151.png" alt="image-20251102152942151" style="zoom:33%;" /><img src="./assets/image-20251102153032924.png" alt="image-20251102153032924" style="zoom: 33%;" />

* å‡å°‘å°†attentionç”¨ä¸€ä¸ªkernelå®Œæˆåï¼Œé€Ÿåº¦æ˜¾è‘—é™ä½
* ä»å³å›¾ä¹Ÿå¯ä»¥çœ‹åˆ°è™½ç„¶matmulå ç°å­˜å¾ˆå¤šï¼Œä½†æ˜¯å› ä¸ºæ˜¯çŸ©é˜µè¿ç®—ï¼Œé€Ÿåº¦å¿«ï¼Œåè€Œæ˜¯dropoutã€softmaxå’Œmaskè¿™äº›åœ°æ–¹é€Ÿåº¦å¾ˆæ…¢

Flash Attentionä¹Ÿæ˜¯ä¸ç”¨çš„æ—¶å€™æŠŠä¸œè¥¿æ”¾åˆ°CPUä¸Š

<img src="./assets/image-20251102153631422.png" alt="image-20251102153631422" style="zoom:33%;" /><img src="./assets/image-20251102154802905.png" alt="image-20251102154802905" style="zoom: 25%;" />

ä¸Šå›¾è¶Šé«˜ä»£è¡¨é€Ÿåº¦è¶Šå¿«

### Liger Kernel

Reimplement some core LLM computation with optimized Triton code.

ä½¿ç”¨ä¹Ÿå¾ˆæ–¹ä¾¿ï¼Œç›´æ¥è°ƒç”¨ç›¸åº”çš„å‡½æ•°å³å¯

<img src="./assets/image-20251102155532808.png" alt="image-20251102155532808" style="zoom:33%;" />

![image-20251102155415358](./assets/image-20251102155415358.png)

## Part III : Quantization

**é‡åŒ–æ˜¯ä¸€ç§æœ‰æŸå‹ç¼©ï¼ˆLossy Compressionï¼‰æŠ€æœ¯**ï¼Œå°†æ¨¡å‹ä¸­çš„é«˜ç²¾åº¦æƒé‡ï¼ˆé€šå¸¸æ˜¯ float32 æˆ– float16ï¼‰è½¬æ¢ä¸ºä½ç²¾åº¦æ•´æ•°ï¼ˆå¦‚ int8ã€int4ï¼‰

![image-20251102155901060](./assets/image-20251102155901060.png)

 `Meta-Llama-3.1-8B-Instruct-Q8_0.gguf`ï¼š 

- `Q8_0` è¡¨ç¤º **æ¯ä¸ªæƒé‡ç”¨ 8-bit æ•´æ•°å­˜å‚¨**
- æ¨¡å‹å¤§å° â‰ˆ 8B Ã— 1 byte = **8 GB**